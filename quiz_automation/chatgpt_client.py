"""OpenAI client wrapper used by the quiz automation project.

This module exposes a small helper class :class:`ChatGPTClient` that wraps the
`openai` package.  It handles a couple of responsibilities:

* Validation and creation of the underlying :class:`~openai.OpenAI` client.
* Basic response caching keyed by a hash of the question text.
* Parsing the JSON payload returned by the model.
* Tracking token usage and estimating cost.

The main entry point is :meth:`ChatGPTClient.ask` which returns a
:class:`ChatGPTResponse` describing the answer returned by the model, the usage
object reported by OpenAI and the calculated monetary cost.
"""

from __future__ import annotations

import json
import time
from dataclasses import dataclass
from typing import Any

from openai import OpenAI

from .config import Settings, get_settings
from .utils import hash_text


@dataclass
class ChatGPTResponse:
    """Container returned by :meth:`ChatGPTClient.ask`.

    Attributes
    ----------
    answer:
        The single letter answer generated by the model.
    usage:
        The token usage information returned by OpenAI or ``None`` if the
        response could not be parsed.
    cost:
        The estimated monetary cost of the request.
    """

    answer: str
    usage: Any | None
    cost: float

    def __iter__(self):
        """Allow unpacking ``ChatGPTResponse`` like a tuple."""
        yield self.answer
        yield self.usage
        yield self.cost


# Default runtime settings and response cache.  The cache maps the hash of the
# question text to the corresponding :class:`ChatGPTResponse`.
settings = get_settings()
CACHE: dict[str, ChatGPTResponse] = {}


class ChatGPTClient:
    """Small wrapper around :class:`openai.OpenAI` used for the quiz bot."""

    def __init__(self, settings: Settings | None = None) -> None:
        self.settings = settings or globals()["settings"]
        if not self.settings.openai_api_key:
            raise ValueError("API key is required")
        # Construct the OpenAI client.
        self.client = OpenAI(api_key=self.settings.openai_api_key)

    def ask(self, question: str) -> ChatGPTResponse:
        """Ask ChatGPT a question and return the parsed answer.

        The method checks a simple in-memory cache before reaching out to the
        OpenAI API.  Requests are retried up to three times with exponential
        backoff.  On successful replies the token usage is logged and the
        response is stored in the cache.
        """

        key = hash_text(question)
        cached = CACHE.get(key)
        if cached is not None:
            return cached

        prompt = (
            f"Answer the quiz question with a single letter in JSON: {question}"
        )

        backoff = 1.0
        for attempt in range(3):
            try:
                completion = self.client.responses.create(
                    model=self.settings.openai_model,
                    temperature=self.settings.openai_temperature,
                    input=prompt,
                )
                break
            except Exception:
                if attempt == 2:
                    return ChatGPTResponse("Error: malformed response", None, 0.0)
                time.sleep(backoff)
                backoff *= 2
        else:  # pragma: no cover - defensive, loop already returns on failure
            return ChatGPTResponse("Error: malformed response", None, 0.0)

        try:
            text = completion.output[0].content[0].text
            data = json.loads(text)
            answer = data.get("answer", "")
        except Exception:
            return ChatGPTResponse("Error: malformed response", None, 0.0)

        usage = getattr(completion, "usage", None)
        input_tokens = getattr(usage, "input_tokens", 0) if usage else 0
        output_tokens = getattr(usage, "output_tokens", 0) if usage else 0

        # Log token usage for debugging/monitoring purposes.
        print(f"Token usage: input={input_tokens} output={output_tokens}")

        cost = (
            input_tokens * self.settings.openai_input_cost
            + output_tokens * self.settings.openai_output_cost
        ) / 1000

        response = ChatGPTResponse(answer, usage, cost)
        CACHE[key] = response
        return response
